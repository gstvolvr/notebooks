{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "It's a framework to represent `reinforcement learning` problems.\n",
    "`state`: $S$ — e.g. each board combo in 2048<br>\n",
    "`model`: $T(s, a, s')$ ~ Pr(s'|s, a) — i.e. the rules of the game that we're playing\n",
    "* it gives us the probability that we'll end up in state $s'$ given that we were in state $s$, and took action $a$\n",
    "\n",
    "`actions`: $A(s), A$ — e.g. up, down, left, and right in 2048<br>\n",
    "`reward`: $R(S)$ — i.e. reward of the value of entering state $s$<br>\n",
    "`policy`: $\\pi(s) \\rightarrow a$ — for any given state that we're in, it determines the action that we should take. \n",
    "\n",
    "We're trying to find $\\pi^*$, the optimal policy. It optimizes are total expected reward. \n",
    "\n",
    "### Properties\n",
    "* `markov property` — only the previous state matters\n",
    "* things are stationary, i.e. the rules don't change\n",
    "\n",
    "Our goal is to find $\\pi^*$\n",
    "$$\\pi^* = \\arg\\max_{\\pi} E\\left[ \\sum_t \\gamma^T R(s_t) | \\pi\\right]$$\n",
    "\n",
    "There's also the notion of `delayed reward` which we can account with using `utility`.\n",
    "$$U^{\\pi}(s) = E\\left[ \\sum_t \\gamma^T R(s_t) | \\pi, S_0 = S\\right]$$\n",
    "\n",
    "It's important to note that $R(s) \\ne U^{\\pi}(s)$\n",
    "\n",
    "Now that we know `utlity`, we can define our strategy at state $s$ by using:\n",
    "$$\\pi^*(s) = \\arg\\max_a \\sum_{s'} T(s, a, s')U(s')$$\n",
    "\n",
    "`utility` factors in the long term aspects, where the `reward` is moment to moment\n",
    "\n",
    "### Bellman's Equation\n",
    "$$U(s) = R(s) + \\lambda max_a \\sum_{s'} T(s, a, s') U(s')$$\n",
    "\n",
    "How do we solve this equation? Well, if there are $N$ states, then there are $N$ unkonwns. Unfortunately, the equation is not linear because of the `max` component. \n",
    "\n",
    "To solve this we follow the following algorithm:<br>\n",
    "1) start with arbitrary utilities<br>\n",
    "2) update utliities based on neighbors<br>\n",
    "3) repeat until convergence\n",
    "\n",
    "### Value Iteration\n",
    "So we'll update (at every iteration) my estimate of the utility at state $s$ by recalculating it to be actual reward I get for entering state $s$, plus the discounted utility that I expect given the original estimates of my utility.\n",
    "$$\\hat{U}_{t+1}(s) = R(s) + \\lambda max_a \\sum_{s'} T(s, a, s')\\hat{U}_t(s')$$\n",
    "\n",
    "We start off with an arbitrary function, and slowly add <i>truth</i> enough times that it converges to the right answer.\n",
    "\n",
    "### Policy Iteration\n",
    "- start with $\\pi_0$ <- guess\n",
    "- evaluate: given $\\pi_t$ calculate $U_t = U^{\\pi_t}$\n",
    "- improve: $\\pi_{t+1} = \\arg\\max_a \\sum T(s, a, s')U_t(s')$\n",
    "\n",
    "We're able to get rid of the `max` and make it a problem of `N` equations and `N` unkonwns. \n",
    "\n",
    "$$U_t(s) = R(s) + \\gamma \\sum T(s, \\pi_z(s), s')U_t(s')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
