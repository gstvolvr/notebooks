{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive LASSO Regression\n",
    "One of the problems with [LASSO](lasso_regression.ipynb) is that the shrinkage in the variables is constant. `Adaptive LASSO` provides a solution by scaling the weights \n",
    "\n",
    "#### Background: Non-Negative Garrote (NNG)\n",
    "The method that inspired `Adaptive LASSO`. We rewrite the `LASSO` problem as\n",
    "$$min_{d_j} \\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^p d_j \\hat{\\beta}_j^{ols} x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p d_j ~;~ d_j \\ge 0$$\n",
    "\n",
    "Where:\n",
    "$$\\hat{\\beta}_j = \\hat{d}_j \\hat{\\beta}_j^{ols}$$\n",
    "\n",
    "So we are scaling the `OLS` solution.\n",
    "\n",
    "The closed form solution in the `orthonormal` case looks like:\n",
    "$$\\beta_j^{garrote} = sign(\\hat{\\beta}_j^{ols}) \\left(|\\hat{\\beta}_j^{ols}| - \\frac{1}{2} \\frac{\\lambda}{|\\hat{\\beta}_j^{ols}|}\\right)_+$$\n",
    "\n",
    "---\n",
    "\n",
    "`Adaptive LASSO` is very similar to `NNG`, but adds a small twist. The weights can be `OLS` estimates, `ridge` estimates, or any other estimates we want. It also has two tuning parameters:\n",
    "* $\\lambda$ — the usual regularization parameter\n",
    "* $\\gamma$  — tuning parameters for the weights\n",
    "\n",
    "$$\\beta^{Alasso} = argmin \\|y - X\\beta\\|_2^2 + \\lambda_n \\sum_{j=1}^p \\hat{w}_j|\\beta_j|$$\n",
    "Where $\\hat{w}_j$ can be defined by:<br/>\n",
    "$$\\hat{w}_j = \\frac{1}{|\\hat{\\beta}_j^{ols}|^{\\gamma}}, \\hat{w}_j = \\frac{1}{|\\hat{\\beta}_j^{ridge}|^{\\gamma}}$$\n",
    "\n",
    "When $\\gamma$ is small $\\dots$\n",
    "\n",
    "There's no direct `adaptive LASSO` implementation in `sklearn`, but one can alter the Lasso linear model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[ 0.  0.  0. ... -0.  0.  0.]\n",
      "702274.329202198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ipanema/src/gstvolvr/notebooks/venv/lib/python3.7/site-packages/ipykernel_launcher.py:35: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/Ipanema/src/gstvolvr/notebooks/venv/lib/python3.7/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan ... nan nan nan]\n",
      "[nan nan nan ... nan nan nan]\n",
      "nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[nan nan nan ... nan nan nan]\n",
      "nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[nan nan nan ... nan nan nan]\n",
      "nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[nan nan nan ... nan nan nan]\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy as sp\n",
    "\n",
    "# data parameters\n",
    "n_samples, n_features = 500, 10000\n",
    "\n",
    "# generate data\n",
    "X, y, coef = make_regression(n_samples=n_samples, n_features=n_features, n_informative=50,\n",
    "    noise=0.1, shuffle=True, coef=True, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# lasso\n",
    "# clf_lasso = Lasso(alpha=1., fit_intercept=False)\n",
    "# clf_lasso.fit(X, y)\n",
    "# coef_lasso = clf_lasso.coef_\n",
    "# y_lasso = clf_lasso.predict(X)\n",
    "# mse_lasso = np.sum((y - y_lasso) ** 2) / n_samples\n",
    "# print(f\"LASSO MSE: {mse_lasso: 0.2f}\") \n",
    "\n",
    "# adaptive lasso\n",
    "lambda_n, gamma = 0.01, 2\n",
    "weights = np.ones(n_features)\n",
    "\n",
    "obj = lambda beta: np.sum((y - X.dot(beta)) ** 2) \\\n",
    "                + lambda_n * 1 / (gamma) \\\n",
    "                * np.sum(np.power(np.abs(beta), gamma))\n",
    "\n",
    "max_lasso_iterations = 5\n",
    "\n",
    "\n",
    "for k in range(max_lasso_iterations):\n",
    "    X_w = np.divide(X, weights[np.newaxis, :])\n",
    "    \n",
    "    clf_lasso = Lasso(alpha=1., fit_intercept=False)\n",
    "    clf_lasso.fit(X_w, y)\n",
    "    coef = clf_lasso.coef_ / weights\n",
    "    weights = np.power(np.abs(coef), gamma)\n",
    "    this_obj = obj(coef)\n",
    "    print(weights)\n",
    "    print(coef)\n",
    "    print(this_obj)\n",
    "    \n",
    "# b_ols = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n",
    "# clf_ridge = Ridge(alpha=0., fit_intercept=False)\n",
    "# clf_ridge.fit(X, y)\n",
    "# b_ridge = clf_ridge.coef_\n",
    "# w1 = 1. / np.abs(b_ols) ** gamma\n",
    "# w2 = 1. / np.abs(b_ridge) ** gamma\n",
    "\n",
    "\n",
    "# lasso = \n",
    "# # initialize \n",
    "# beta = np.zeros(n_features)\n",
    "\n",
    "# obj = lambda b: (y - X.dot(b)) + lambda_n * np.sqrt(np.abs(b)) / np.sqrt(np.abs(b)) ** gamma\n",
    "\n",
    "# n_lasso_iterations = 5\n",
    "# # p_obj = lambda w: 1. / (2 * n_samples) * np.sum((y - np.dot(X, w)) ** 2) \\\n",
    "# #                   + alpha * np.sum(g(w))\n",
    "\n",
    "# # obj = \n",
    "# y - X.dot(beta)\n",
    "# obj(beta)\n",
    "\n",
    "# for k in range(n_lasso_iterations):\n",
    "    \n",
    "#     clf = Ridge(alpha=lambda_n, fit_intercept=False)\n",
    "#     clf.fit(X, y)\n",
    "#     beta = clf.coef_ + lambda_n * beta * np.sqrt(np.abs(clf.coef_))\n",
    "#     print(obj(beta))\n",
    "    \n",
    "#     beta = \n",
    "#     print(beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.power(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.73906715e+10, -1.44204607e+12,  1.01004427e+12, ...,\n",
       "        -1.09315461e+12, -1.44065171e+12,  1.63715711e+12],\n",
       "       [ 1.38555123e+12,  3.18984882e+12,  4.49321081e+12, ...,\n",
       "         1.05019970e+12,  7.02268755e+12, -1.64279845e+12],\n",
       "       [ 4.74946914e+11,  4.50926820e+11,  1.77214422e+12, ...,\n",
       "        -1.55651407e+10,  1.96737158e+12,  1.11744986e+10],\n",
       "       ...,\n",
       "       [ 1.77300723e+11, -2.06580334e+12, -4.45934464e+11, ...,\n",
       "        -1.76326910e+12, -3.37477630e+12,  2.80550137e+12],\n",
       "       [ 9.83744435e+10, -1.45988123e+12,  1.10685249e+12, ...,\n",
       "        -2.57129013e+12, -2.31474264e+12,  3.89655876e+12],\n",
       "       [-1.41876401e+12, -3.98964033e+12, -2.20928711e+12, ...,\n",
       "        -2.81432377e+12, -7.98859426e+12,  5.30809085e+12]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(X.T.dot(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1233792., -2224128.,   801984., ...,    87168.,   -42752.,\n",
       "         267776.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204333631.98105326\n",
      "240461623.17095697\n",
      "240520635.75959608\n",
      "240525705.67622042\n",
      "240525610.83344257\n",
      "240525610.79024196\n",
      "240525610.79006127\n",
      "240525610.79006013\n",
      "240525610.7900602\n",
      "240525610.7900602\n"
     ]
    }
   ],
   "source": [
    "## credit: Alexandre Gramfort — https://gist.github.com/agramfort/1610922\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "n_samples, n_features = 500, 10000\n",
    "X, y, coef = make_regression(n_samples=n_samples, n_features=n_features, n_informative=50,\n",
    "    noise=0.1, shuffle=True, coef=True, random_state=42)\n",
    "\n",
    "X /= np.sum(X ** 2, axis=0)\n",
    "\n",
    "lambda_n = 0.01\n",
    "\n",
    "g = lambda w: np.sqrt(np.abs(w))\n",
    "gprime = lambda beta: 1. / (2. * np.sqrt(np.abs(beta)) + np.finfo(float).eps)\n",
    "\n",
    "p = lambda beta: np.power(beta, gamma)\n",
    "p_prime = lambda beta: gamma * np.power(beta, gamma - 1)\n",
    "\n",
    "obj = lambda beta: np.sum((y - X.dot(beta)) ** 2) \\\n",
    "                + lambda_n * 1 / (gamma) \\\n",
    "                * np.sum(np.power(np.abs(beta), gamma))\n",
    "\n",
    "weights = np.ones(n_features)\n",
    "n_lasso_iterations = 10\n",
    "train_score = np.zeros(n_lasso_iterations)\n",
    "\n",
    "def loss(y, pred):\n",
    "    return 1. / (2 * y.shape[0]) \\\n",
    "            * np.sum(np.square(y - pred)) \\\n",
    "            + lambda_n * np.sum(p(np.abs(coef)))\n",
    "    \n",
    "for k in range(n_lasso_iterations):\n",
    "    X_w = X / weights.reshape(1, -1)\n",
    "    clf = Lasso(alpha=lambda_n, fit_intercept=False)\n",
    "    clf.fit(X_w, y)\n",
    "    coef_ = clf.coef_ / weights\n",
    "    weights = gprime(coef_)\n",
    "    pred = X.dot(coef)\n",
    "    train_score[k] = loss(y, pred)\n",
    "    print(obj(coef_)) # should go down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([98497.43727081, 98497.43727081, 98497.43727081, 98497.43727081,\n",
       "       98497.43727081, 98497.43727081, 98497.43727081, 98497.43727081,\n",
       "       98497.43727081, 98497.43727081])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective:  12255.7233\n",
      "loss:  17115.6500\n",
      "\n",
      "objective: -985.1588\n",
      "loss:  3869.8042\n",
      "\n",
      "objective: -985.2592\n",
      "loss:  3869.6956\n",
      "\n",
      "objective: -985.2608\n",
      "loss:  3869.7064\n",
      "\n",
      "objective: -985.2631\n",
      "loss:  3869.7204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X, y, coef = make_regression(n_samples=306, n_features=8000, n_informative=50,\n",
    "                    noise=0.1, shuffle=True, coef=True, random_state=42)\n",
    "\n",
    "X /= np.sum(X ** 2, axis=0)  # scale features\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 2\n",
    "\n",
    "# g = lambda w: np.sqrt(np.abs(w))\n",
    "# gprime = lambda w: 1. / (2. * np.sqrt(np.abs(w)) + np.finfo(float).eps)\n",
    "# g = lambda w: np.power(w, gamma)\n",
    "# # gprime = lambda w: gamma * np.power(w, gamma - 1 + np.finfo(float).eps)\n",
    "# gprime = lambda w: gamma / (np.power(w, 1 - gamma) + np.finfo(float).eps)\n",
    "\n",
    "# Or another option:\n",
    "ll = 0.01\n",
    "g = lambda w: np.log(ll + np.abs(w))\n",
    "gprime = lambda w: 1. / (ll + np.abs(w))\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "p_obj = lambda w: 1. / (2 * n_samples) * np.sum((y - np.dot(X, w)) ** 2) \\\n",
    "                  + alpha * np.sum(g(w))\n",
    "\n",
    "weights = np.ones(n_features)\n",
    "n_lasso_iterations = 5\n",
    "\n",
    "def loss(y, pred):\n",
    "    return 1. / (2 * y.shape[0]) \\\n",
    "            * np.sum(np.square(y - pred)) \\\n",
    "            + lambda_n * np.sum(p(np.abs(coef)))\n",
    "\n",
    "for k in range(n_lasso_iterations):\n",
    "    X_w = X / weights[np.newaxis, :]\n",
    "    clf = Lasso(alpha=alpha, fit_intercept=False)\n",
    "    clf.fit(X_w, y)\n",
    "    coef_ = clf.coef_ / weights\n",
    "    preds = X.dot(coef_)\n",
    "    weights = gprime(coef_)\n",
    "    print(f'objective: {p_obj(coef_): 0.4f}')\n",
    "    print(f'loss: {loss(y, preds): 0.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3872.356231517805"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = X.dot(coef_)\n",
    "loss(y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources:\n",
    "* [ISyE8803: Topics on High-Dimensional Data Analytics — Module 4](https://courses.edx.org/courses/course-v1:GTx+ISYE8803+2T2019/course/#block-v1:GTx+ISYE8803+2T2019+type@chapter+block@223667a47f58432ea40f272c8ed71e11)\n",
    "* [Alexandre Gramfort — Demo Adaptive LASSO](https://gist.github.com/agramfort/1610922)\n",
    "* [Adaptive LASSO — sklearn pull request](https://github.com/henridwyer/scikit-learn/blob/c8b2c94e6335405e29fa411378d5d27c78808a6d/sklearn/linear_model/coordinate_descent.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
