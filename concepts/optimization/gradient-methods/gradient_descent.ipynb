{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "A `first order method`. Assume `f` is a continuous and twice diferentiable function, and we want to solve: $\\min_{x} f(x)$\n",
    "\n",
    "An intuitive approach is to start at some initial point, and iteratively move in the direction that decreases `f`.<br>\n",
    "\n",
    "A natural choice for the direction, is the negative [gradient](../calculus/gradients.ipynb): <br>\n",
    "$x^{(k+1)} = x^{(k)} - t_k \\nabla f(x^{(k)})$ where $t_k$ is a step size\n",
    "\n",
    "\n",
    "the algorithm is as follows:<br>\n",
    "#### English version\n",
    "1. choose an initial solution $x^0$\n",
    "2. choose a descent direction $d^0$\n",
    "3. choose a step size $\\alpha_0 > 0$\n",
    "4. update the solution $x^1 = x^0 + \\alpha_0 d^0$\n",
    "5. if some stopping criteria is met, `stop`, else repeat with current solution. \n",
    "\n",
    "#### Math version\n",
    "---\n",
    "1: guess $x^{(0)}$, set k = 0<br>\n",
    "2: while ||$\\nabla f(x^{(k)})|| \\geq \\epsilon$ do <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3: $x^{(k+1)} = x^{(k)} - t_k \\nabla f(x^{(k)})$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4: k += 1<br>\n",
    "5: end while<br>\n",
    "6: return $x^{(k)}$<br>\n",
    "\n",
    "\n",
    "Let $x^k$ be the current iteration, and we want to choose a \"downhill\" direction $d^k$ and a step size $\\alpha$ such that $f(x^k + \\alpha d^k) < f(x^k)$\n",
    "\n",
    "By [Taylor's approximation](../../calculus/taylors_approximation.ipynb):\n",
    "$$f(x^k + \\alpha d^k) \\approx f(x^k) + \\alpha \\nabla f(x^k)^T d_k$$\n",
    "\n",
    "So we want $\\nabla f(x^k)^Td^k < 0$. The steepest descent direction is $d^k = -\\nabla f(x^k)$\n",
    "\n",
    "---\n",
    "\n",
    "We know what direction to move in, but how much should we move by? What should the step size be? \n",
    "\n",
    "* line search: define $g(\\alpha) := f(x^k + \\alpha d^k)$. Choose $\\alpha$ to minimize $g$\n",
    "* fixed step size: Fir $\\alpha$ a priori (may not converge if $\\alpha$ is too big\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's look at this algo with a simple example: $x^2$\n",
    "\n",
    "We know from the start that $\\nabla f(x) = 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random initialization: [592]\n",
      "[296.]\n",
      "[592] => [296.]\n",
      "[148.]\n",
      "[296.] => [148.]\n",
      "[74.]\n",
      "[148.] => [74.]\n",
      "[37.]\n",
      "[74.] => [37.]\n",
      "[18.5]\n",
      "[37.] => [18.5]\n",
      "[9.25]\n",
      "[18.5] => [9.25]\n",
      "[4.625]\n",
      "[9.25] => [4.625]\n",
      "[2.3125]\n",
      "[4.625] => [2.3125]\n",
      "[1.15625]\n",
      "[2.3125] => [1.15625]\n",
      "[0.578125]\n",
      "[1.15625] => [0.578125]\n",
      "[0.2890625]\n",
      "[0.578125] => [0.2890625]\n",
      "finished in 11 steps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, gf, tk, size=1):\n",
    "    \"\"\"\n",
    "    :parameters\n",
    "    f: function\n",
    "    gf: gradient\n",
    "    tk: step size\n",
    "    \"\"\"\n",
    "    x = np.random.randint(0, 1000, size=size) \n",
    "    print(f\"random initialization: {x}\")\n",
    "    eps = 1.0\n",
    "    steps = 0\n",
    "    \n",
    "    while np.all(np.abs(gf(x)) >= eps):\n",
    "        x_new = x - tk * gf(x)\n",
    "        print(x_new)\n",
    "        print(f\"{x} => {x_new}\")\n",
    "        x = x_new\n",
    "        steps += 1\n",
    "    print(f\"finished in {steps} steps\")\n",
    "\n",
    "tk = 0.25\n",
    "gradient_descent(lambda x: x ** 2, lambda x: 2 * x, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How quickly we find the solution depends on the step size. We can alter our current approach to adoptively adjust the step size. \n",
    "\n",
    "<strong>Extact line search:</strong><br>\n",
    "In each iteration choose the step that minimizes $f(x^{(k+1)})$\n",
    "\n",
    "$\\arg\\min_{t\\geq0} f(x^{(k)} - t\\nabla f(x^{(k)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1600.0, 1024.0, 576.0, 255.9999999999999, 64.0, 0.0, 64.00000000000011, 256.0000000000002, 576.0, 1024.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x: x ** 2\n",
    "gf = lambda x: 2 * x\n",
    "x = np.random.randint(0, 1000)\n",
    "res = list(map(lambda t: f(x - t * gf(x)), np.arange(0, 1, 0.1)))\n",
    "print(res)\n",
    "np.argmin(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random initialization: 543\n",
      "chosen step: 0.5\n",
      "543.0000 => 0.0000\n",
      "finished in 1 steps\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent_line_search(f, gf):\n",
    "    x = np.random.randint(0, 1000) \n",
    "    print(f\"random initialization: {x}\")\n",
    "    eps = 1.0\n",
    "    steps = 0\n",
    "    tk = 1\n",
    "    tk_range = np.arange(0, 1.0, 0.1)\n",
    "    \n",
    "    while np.abs(gf(x)) >= eps:\n",
    "        tk_i = np.argmin(list(map(lambda t: f(x - t * gf(x)), tk_range)))\n",
    "        tk = tk_range[tk_i]\n",
    "        print(f\"chosen step: {tk}\")\n",
    "        x_new = x - tk * gf(x)\n",
    "        print(f\"{x:.4f} => {x_new:.4f}\")\n",
    "        x = x_new\n",
    "        steps += 1\n",
    "    print(f\"finished in {steps} steps\")\n",
    "\n",
    "gradient_descent_line_search(lambda x: x ** 2, lambda x: 2 * x)\n",
    "\n",
    "f_ex = lambda x1, x2: 4*x1**2 + 2*x2**2 - 4*x1*x2\n",
    "gf_ex = np.array([lambda x1, x2: 8*x1 - 4*x2, \n",
    "                  lambda x1, x2: 4*x2 - 4*x1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Backtracking line search:</strong><br>\n",
    "Start with an initial t and then in iteration $k$, use $\\frac{t}{2^{(k-1)}}$ or in general $t^*C$ where $C\\in(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random initialization: 846\n",
      "chosen step: 0.1044\n",
      "846.0000 => 669.3692\n",
      "chosen step: 0.7397\n",
      "669.3692 => -320.9215\n",
      "chosen step: 0.6443\n",
      "-320.9215 => 92.6214\n",
      "chosen step: 0.6007\n",
      "92.6214 => -18.6573\n",
      "chosen step: 0.4067\n",
      "-18.6573 => -3.4821\n",
      "chosen step: 0.8668\n",
      "-3.4821 => 2.5547\n",
      "chosen step: 0.5018\n",
      "2.5547 => -0.0091\n",
      "finished in 7 steps\n"
     ]
    }
   ],
   "source": [
    "def find_step(f, gf, x, c=0.9):\n",
    "    t = np.random.rand()\n",
    "    \n",
    "    while(f(x + t * gf(x)) < f(x)):\n",
    "        t = t * c # backtracking blind search\n",
    "    return t\n",
    "    \n",
    "\n",
    "def gradient_descent_exact_line(f, gf):\n",
    "    x = np.random.randint(0, 1000) \n",
    "    print(f\"random initialization: {x}\")\n",
    "    eps = 1.0\n",
    "    steps = 0\n",
    "    tk = 1\n",
    "    \n",
    "    while np.abs(gf(x)) >= eps:\n",
    "        tk = find_step(f, gf, x)\n",
    "        print(f\"chosen step: {tk:0.4f}\")\n",
    "        x_new = x - tk * gf(x)\n",
    "        print(f\"{x:.4f} => {x_new:.4f}\")\n",
    "        x = x_new\n",
    "        steps += 1\n",
    "    print(f\"finished in {steps} steps\")\n",
    "    \n",
    "gradient_descent_exact_line(lambda x: x ** 2, lambda x: 2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Another example\n",
    "\n",
    "$$\\min f(x) = (x_1 + 1)^4 + x_1x_2 + (x_2 + 1)^4$$\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$\\nabla f(x) = \\begin{bmatrix}\n",
    "       \\frac{\\partial f}{\\partial x_1} \\\\\n",
    "       \\frac{\\partial f}{\\partial x_2}\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1} = 4(x_1 + 1)^3 + x_2$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_2} = x_1 + 4(x_2 + 1)^3$$\n",
    "\n",
    "\n",
    "$$\\nabla f(x) = \\begin{bmatrix}\n",
    "       4(x_1 + 1)^3 + x_2 \\\\\n",
    "       x_1 + 4(x_2 + 1)^3\n",
    "   \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random initialization: [287 313]\n",
      "[-23887663.25 -30958902.75]\n",
      "[287 313] => [-23887663.25 -30958902.75]\n",
      "[1.36307876e+22 2.96726708e+22]\n",
      "[-23887663.25 -30958902.75] => [1.36307876e+22 2.96726708e+22]\n",
      "[-2.53257811e+66 -2.61258190e+67]\n",
      "[1.36307876e+22 2.96726708e+22] => [-2.53257811e+66 -2.61258190e+67]\n",
      "[1.62438342e+199 1.78323976e+202]\n",
      "[-2.53257811e+66 -2.61258190e+67] => [1.62438342e+199 1.78323976e+202]\n",
      "[-inf -inf]\n",
      "[1.62438342e+199 1.78323976e+202] => [-inf -inf]\n",
      "[nan nan]\n",
      "[-inf -inf] => [nan nan]\n",
      "finished in 6 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ipanema/src/gstvolvr/notebooks/venv/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in double_scalars\n",
      "  \n",
      "/Users/Ipanema/src/gstvolvr/notebooks/venv/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/Ipanema/src/gstvolvr/notebooks/venv/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in subtract\n",
      "  app.launch_new_instance()\n",
      "/Users/Ipanema/src/gstvolvr/notebooks/venv/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def f(x): return (x[0] + 1) ** 4 + x[0]*x[1] + (x[1] + 1)**4\n",
    "def pg1(x): return 4 * (x[0] + 1) ** 3 + x[1]\n",
    "def pg2(x): return x[0] + 4 * (x[1] + 1) ** 3\n",
    "def gf(x): return np.array([pg1(x),pg2(x)])\n",
    "\n",
    "gradient_descent(f, gf, tk, size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources:\n",
    "* [ISyE 6669 Discrete Optimization â€” Gradient Descent](https://courses.edx.org/courses/course-v1:GTx+ISYE6669+2T2019/courseware/4f017d33a98749118de2413c8a8e4660/5131388fa021401a88546a6414af852e/1?activate_block_id=block-v1%3AGTx%2BISYE6669%2B2T2019%2Btype%40vertical%2Bblock%40ac9c0cad50ca493299d2832467f26b04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
