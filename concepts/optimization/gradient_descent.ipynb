{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "A [first order](overview.ipynb#First-Order-Methods) method. Assume `f` is a continuous and twice diferentiable function, and we want to solve: $min_{x} f(x)$\n",
    "\n",
    "An intuitive approach is to start at some initial point, and iteratively move in the direction that decreases `f`.<br>\n",
    "A natural choice in the direction, is the negative [gradient](../calculus/gradients.ipynb): <br>\n",
    "$x^{(k+1)} = x^{(k)} - t_k \\nabla f(x^{(k)})$ where $t_k$ is a step size\n",
    "\n",
    "\n",
    "the algorithm is as follows:<br>\n",
    "\n",
    "1: guess $x^{(0)}$, set k = 0<br>\n",
    "2: while ||$\\nabla f(x^{(k)})|| \\geq \\epsilon$ do <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3: $x^{(k+1)} = x^{(k)} - t_k \\nabla f(x^{(k)})$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4: k += 1<br>\n",
    "5: end while<br>\n",
    "6: return $x^{(k)}$<br>\n",
    "\n",
    "Let's look at this algo with a simple example: $x^2$\n",
    "\n",
    "We know from the start that $\\nabla f(x) = 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random initialization: 973\n",
      "973.0000 => 486.5000\n",
      "486.5000 => 243.2500\n",
      "243.2500 => 121.6250\n",
      "121.6250 => 60.8125\n",
      "60.8125 => 30.4062\n",
      "30.4062 => 15.2031\n",
      "15.2031 => 7.6016\n",
      "7.6016 => 3.8008\n",
      "3.8008 => 1.9004\n",
      "1.9004 => 0.9502\n",
      "0.9502 => 0.4751\n",
      "finished in 11 steps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, gf, tk):\n",
    "    x = np.random.randint(0, 1000) \n",
    "    print(f\"random initialization: {x}\")\n",
    "    eps = 1.0\n",
    "    steps = 0\n",
    "    \n",
    "    while np.abs(gf(x)) >= eps:\n",
    "        x_new = x - tk * gf(x)\n",
    "        print(f\"{x:.4f} => {x_new:.4f}\")\n",
    "        x = x_new\n",
    "        steps += 1\n",
    "    print(f\"finished in {steps} steps\")\n",
    "\n",
    "tk = 0.25\n",
    "gradient_descent(lambda x: x ** 2, lambda x: 2 * x, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How quickly we find the solution depends on the step size. We can alter our current approach to adoptively adjust the step size. \n",
    "\n",
    "<strong>Extact line search:</strong><br>\n",
    "In each iteration choose the step that minimizes $f(x^{(k+1)})$\n",
    "\n",
    "$argmin_{t\\geq0} f(x^{(k)} - t\\nabla f(x^{(k)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1600.0, 1024.0, 576.0, 255.9999999999999, 64.0, 0.0, 64.00000000000011, 256.0000000000002, 576.0, 1024.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x: x ** 2\n",
    "gf = lambda x: 2 * x\n",
    "x = np.random.randint(0, 1000)\n",
    "res = list(map(lambda t: f(x - t * gf(x)), np.arange(0, 1, 0.1)))\n",
    "print(res)\n",
    "np.argmin(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random initialization: 543\n",
      "chosen step: 0.5\n",
      "543.0000 => 0.0000\n",
      "finished in 1 steps\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent_line_search(f, gf):\n",
    "    x = np.random.randint(0, 1000) \n",
    "    print(f\"random initialization: {x}\")\n",
    "    eps = 1.0\n",
    "    steps = 0\n",
    "    tk = 1\n",
    "    tk_range = np.arange(0, 1.0, 0.1)\n",
    "    \n",
    "    while np.abs(gf(x)) >= eps:\n",
    "        tk_i = np.argmin(list(map(lambda t: f(x - t * gf(x)), tk_range)))\n",
    "        tk = tk_range[tk_i]\n",
    "        print(f\"chosen step: {tk}\")\n",
    "        x_new = x - tk * gf(x)\n",
    "        print(f\"{x:.4f} => {x_new:.4f}\")\n",
    "        x = x_new\n",
    "        steps += 1\n",
    "    print(f\"finished in {steps} steps\")\n",
    "\n",
    "gradient_descent_line_search(lambda x: x ** 2, lambda x: 2 * x)\n",
    "\n",
    "f_ex = lambda x1, x2: 4*x1**2 + 2*x2**2 - 4*x1*x2\n",
    "gf_ex = np.array([lambda x1, x2: 8*x1 - 4*x2, \n",
    "                  lambda x1, x2: 4*x2 - 4*x1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Backtracking line search:</strong><br>\n",
    "Start with an initial t and then in iteration $k$, use $\\frac{t}{2^{(k-1)}}$ or in general $t^*C$ where $C\\in(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random initialization: 846\n",
      "chosen step: 0.1044\n",
      "846.0000 => 669.3692\n",
      "chosen step: 0.7397\n",
      "669.3692 => -320.9215\n",
      "chosen step: 0.6443\n",
      "-320.9215 => 92.6214\n",
      "chosen step: 0.6007\n",
      "92.6214 => -18.6573\n",
      "chosen step: 0.4067\n",
      "-18.6573 => -3.4821\n",
      "chosen step: 0.8668\n",
      "-3.4821 => 2.5547\n",
      "chosen step: 0.5018\n",
      "2.5547 => -0.0091\n",
      "finished in 7 steps\n"
     ]
    }
   ],
   "source": [
    "def find_step(f, gf, x, c=0.9):\n",
    "    t = np.random.rand()\n",
    "    \n",
    "    while(f(x + t * gf(x)) < f(x)):\n",
    "        t = t * c # backtracking blind search\n",
    "    return t\n",
    "    \n",
    "\n",
    "def gradient_descent_exact_line(f, gf):\n",
    "    x = np.random.randint(0, 1000) \n",
    "    print(f\"random initialization: {x}\")\n",
    "    eps = 1.0\n",
    "    steps = 0\n",
    "    tk = 1\n",
    "    \n",
    "    while np.abs(gf(x)) >= eps:\n",
    "        tk = find_step(f, gf, x)\n",
    "        print(f\"chosen step: {tk:0.4f}\")\n",
    "        x_new = x - tk * gf(x)\n",
    "        print(f\"{x:.4f} => {x_new:.4f}\")\n",
    "        x = x_new\n",
    "        steps += 1\n",
    "    print(f\"finished in {steps} steps\")\n",
    "    \n",
    "gradient_descent_exact_line(lambda x: x ** 2, lambda x: 2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
