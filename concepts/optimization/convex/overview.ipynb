{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "What is optimization? It's finding the `min` or `max` of some `objective function` given a set of constraints. \n",
    "For example...\n",
    "\n",
    "$$argmin_x(f_0(x))~s.t.~f_i(x) \\leq 0, i = 1, ..., k$$\n",
    "\n",
    "Most analytics and ML models are in the form of an optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous\n",
    "Includes `convex` and `non-convex` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convex Set\n",
    "contains line segments between any two points in the set.\n",
    "\n",
    "$x_1, x_2 \\in C, 0 \\leq \\theta \\leq 1 => \\theta x_1 + (1 - \\theta)x_2 \\in C$\n",
    "\n",
    "#### Convex Function \n",
    "`f` is convex if the domf is a convex set and\n",
    "$f(\\theta x + (1 - \\theta) y ) \\leq \\theta f(x) + (1 - \\theta)f(y)$ for all $x, y \\in domf, 0 \\leq \\theta \\leq 1$\n",
    "\n",
    "#### Strictly Convex \n",
    "`f` is strictly convex if the domf is a convex set and \n",
    "$f(\\theta x + (1 - \\theta) y ) \\leq \\theta f(x) + (1 - \\theta)f(y)$ for all $x, y \\in domf, 0 \\lt \\theta \\lt 1$\n",
    "\n",
    "\n",
    "How do we verify if a function is convex or not? Well, if the function is differentiable we can find it's first order approximation using `taylor theorem`. We can say that `f` is convex iff for all feasible `x` and `y` values the following is true:\n",
    "\n",
    "$f(y) \\geq + \\nabla f(x)^T(y-x)$\n",
    "\n",
    "#### Properties of convex functions\n",
    "* any locally optimal point, is globally optimal\n",
    "* `x` is optimal iff $\\nabla f(x)^T(y - x) \\geq 0$. There is no direction that could reduce the value of the function, thus it will be optimal. \n",
    "\n",
    "\n",
    "## Set Properties\n",
    "\n",
    "### Closed Set\n",
    "* A set if `closed` if it includes `bounary points`. \n",
    "* Formatlly, a set $X$ is closed if for any convergent sequence in $X$, its limit point also belongs to X, i.e. if $\\{x^i\\}\\in X$ and $lim_{i\\rightarrow \\inf}x^i = x^0$ then $x^0 \\in X$\n",
    " * $X = \\mathbb{R}$ is `closed`\n",
    " * $X = {x: 0 \\lt x \\le 1}$ is `not closed`\n",
    "* If non of the inequalities are strict, then the set is closed. \n",
    "\n",
    "### Bounded Set\n",
    "* A set is bounded if it can be enclosed in a large enough box.\n",
    "* Formally, the set $X$ is bounded if there exists $M\\ge0$ such that $||X|| \\le M$ for all $x \\in X$\n",
    "* A set that is both bounded and closed is called compact. \n",
    "\n",
    "\n",
    "#### Examples\n",
    "* $X = \\mathbb{R}^2$ is `closed` but `not bounded`\n",
    "* $X = \\{(x, y): x^2+y^2 \\lt 1\\}$ is `bounded` but `not closed`\n",
    "* $X = \\{(x, y): x^2+y^2 \\ge 1\\}$ is `closed` but `not bounded`\n",
    "* $X = \\{(x, y): x^2+y^2 \\le 1\\}$ is `closed` and `bounded` (compact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Order Methods\n",
    "Using first derivative to find optimal solution. Sometimes, these functions have `closed form solutions`, similar to `linear regression`, where we can take the first derivative, set it to 0, and find the point that minimizes the function. Below are some options for when we don't have closed form solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence rate for `GD` is $\\frac{1}{k}$ which is sublinear. It can be improved with `Accelerated Gradient Descent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent \n",
    "Consdier $F(x) = \\sum_{i=1}^n f_i(x)$ with convex $f_i(x)$. To estimate the mean of the population a natural loss function to be minimized if $F(x) = \\sum_{i=1}^n (y_i - x)^2$\n",
    "\n",
    "Using `gradient descent` could be very expensive since it would have to calculate the gradient for every function. Stochatic gradient descent would be a better solution for this problem. \n",
    "\n",
    "1: initialize $x_1$<br>\n",
    "2: for k = 1 to $K$ do<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3: fot i = 1 to n do<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4: sample an observation $i$ uniformly at random<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;5: update $x_{k+1} = x_k - \\alpha \\nabla f_i(x_k)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;6: end for\n",
    "7: end for<br>\n",
    "8: return $x_k$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Order Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broyder-Fletcher-Goldfarb-Shanno (BFGS) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
