{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3 Learning Decision Trees\n",
    "A **decision tree** represents a function that takes as input a vector of attribute values an returns a single output value (discrete or continuous). \n",
    "\n",
    "A **decision tree** performs a sequence of tests. Each node corresponds to a test of the value of one of the input attributes, and the branches are labeled with possible values of that attribute. Each _leaf node_ represents a value that can be returned. \n",
    "\n",
    "## 18.3.3 Inducing decision trees from examples\n",
    "We pass in examples as $(\\mathbf{x}, y)$ pairs, and want to get the smallest possible tree that is consistent with those examples. This is an intractable problem given that for a binary problem we'd have to search through $2^{2^n}$ trees. \n",
    "\n",
    "Instead we use heuristics to come with approximations. Decision tree learning uses a greedy divide-and-conquer strategy: always test the most importnat attribute first – i.e. the attribute that makes most difference to the classification of an example.  \n",
    "\n",
    "They represent the pseudocode as follows:\n",
    "\n",
    "```\n",
    "function Decision-Tree-Learning(examples, attributes, parent_examples) return a tree\n",
    "    if examples is empty then return Plurality-Value(parent_examples)\n",
    "    else if all example haev the sample classification then return the classification\n",
    "    else if attributes is empty then return Plurality-Value(examples)\n",
    "    else\n",
    "        A <- argmax Importance(a, examples) # get the more important attribute\n",
    "        tree <- a new decision tree with root test A\n",
    "        for each value v of A do\n",
    "            exs <- {e: e in example and e.A = v}\n",
    "            subtree <- Decision-Tree-Learning(exs, attribuets - A, examples)\n",
    "            add a branch to tree with label (A = v) and subtree subtree\n",
    "        return tree\n",
    "```\n",
    "\n",
    "Finding the best split points is by far the most time consuming part of decision tree learning. \n",
    "\n",
    "\n",
    "## 18.3.4 Choosing attribute tests\n",
    "\n",
    "We need a way to compare features and determine how _useful_ they are. One common approach is to use **entropy** a concept from [information theory](https://github.com/gstvolvr/notebooks/tree/master/concepts/information-theory). \n",
    "\n",
    "Entropy is a measure of the uncertainty of a random variable – the more information we have, the less the entropy. \n",
    "\n",
    "If the random variable is constant (e.g. a double faced coin) then entrop is **0**. If the random variable has 2 states that are equally likely (e.g. an unbiased coin) then we say that it has _1 bit_ of entropy. \n",
    "\n",
    "\n",
    "$$\n",
    "Entropy: H(V) = \\sum_k P(v_k) log_2 \\frac{1}{P(v_k)} = - \\sum_k P(v_k) log_2 P(v_k)\n",
    "$$\n",
    "\n",
    "\n",
    "So if we look at the entropy of a fair coint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-(.5*math.log2(.5) + .5*math.log2(.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the entropy of a coin that gives heads `99%` of the time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08079313589591118"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(.99*math.log2(.99) + .01*math.log2(.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For boolean random variables, the equation can be simplified:\n",
    "$$\n",
    "B(q) = -(q~log_2~q + (1-q)~log_2~(1-q))\n",
    "$$\n",
    "\n",
    "## 18.3.5 Generalization and overfitting\n",
    "On certain problems, the decision tree learner will create a large tree when in reality there is no pattern to be found. This is known as **overfitting**.\n",
    "\n",
    "Overfitting becomes more likely as the hypothesis space and the number of input attributes grows, and less likely as we increase the number of training examples. \n",
    "\n",
    "We can use **decision tree pruning** to combat overfitting. It removes nodes that don't seem relevant. To determine this we use a statistical **significance test**. The **null hypothesis** is that there is no underlying pattern. \n",
    "\n",
    "\n",
    "## 18.10 Ensemble Learning\n",
    "The idea behind **ensemble learning** is to select a collection of hypotheses from the hypothesis space and combine their predictions somehow. We take what are considered _weak learners_ and make a stronger model from their aggregate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
