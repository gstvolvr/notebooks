{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ANN with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "Based on an artificial neuron called the <em>threshold logical unit</em>(TLU). The TLU computes a weighted sum of it's inputs:\n",
    "\n",
    "$z = w_{1}x_{1} + ... + w_{n}x_{n} = \\textbf{x}^T\\textbf{w}$ = z\n",
    "\n",
    "then applies a step function to that sum an output the result:\n",
    "\n",
    "$h_{\\textbf{x}}(\\textbf{x}) = step(z)$<br>\n",
    "\n",
    "It's composed of a layer of TLU's\n",
    "\n",
    "$h_{\\textbf{W,b}}(\\textbf{X}) = \\phi(\\textbf{XW} + \\textbf{b})$<br>\n",
    "\n",
    "$\\textbf{X}$ – is the input features<br>\n",
    "$\\textbf{W}$ – has one row / input neuron and one column / artificial neuron in the layer<br>\n",
    "$\\textbf{b}$ – bias vector that contains all the connection weights between the bias neuron and the artificial neurons.<br>\n",
    "$\\phi$ – is the activation function – when the artificial neuron's are TLU's then it's just a step function<br>\n",
    "\n",
    "Hebb's rule: the connection weight between neuron's is increased whenever they have the same output.\n",
    "\n",
    "### Perceptron training rule\n",
    "$w^{next step}_{i,j} = w_{i,j} + \\eta(y_j - \\hat{y}_j)x_i$<br>\n",
    "\n",
    "$w_{i, j}$ is the connection weight between the ith input neuron and the jth output neuron.<br>\n",
    "$x_i$ is the ith input value of the current training instance.<br>\n",
    "$\\hat{y}_j$ is the output of the jth output neuron for the current training instance.<br>\n",
    "$y_j$ is the target output of the jth output neuron for the current training instance.<br>\n",
    "$\\eta$ is the learning rate.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "per_clf = Perceptron(max_iter=50, tol=None)\n",
    "per_clf.fit(X, y)\n",
    "per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron and backpropagation\n",
    "An MLP is composed of one input layer, one or more layers of TLU's (i.e. hidden layers), and one output layer. Every layer except the output layer inclused a bias neuron and is fully connected to the next layer. \n",
    "\n",
    "### Backpropagation\n",
    "It's Gradient Descent using an efficient technique for computing the gradients automatically in two passes through the network. Backpropagation is able to calculate the gradient of the network's error with regards to every single model parameter – it can find how each connection weights and bias term should be tweaked to reduce the error. Once it has these gradients, it performs a normal gradient descent step and the process is repeated until convergence. \n",
    "\n",
    "For each training instance, the algorithm does a forward pass over the network, and calculates performance with a loss function. It then goes back through the network (one layer at a time) to measure the error contribution from each connection, and tries to tweak the weights to reduce the error (gradient descent step). \n",
    "\n",
    "To make this work, the author's replaced the step function with the logistic function $\\sigma(z) = \\frac{1}{1+e^{-z}}$. The step function had only segments (i.e. there was no slope to compute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
